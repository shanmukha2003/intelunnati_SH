# -*- coding: utf-8 -*-
"""intel unnati.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zUdy7cBpgJS--ZEjxGSSJ-rz0Hk02nWh
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
plt.rcParams['figure.figsize'] = [12, 5]

df = pd.read_csv("/content/train.csv")

df.head()

df.shape

df.isna().sum()

plt.scatter(x=df['Price_Range'],y=df['Ram'])

plt.scatter(x=df['Price_Range'],y=df['Battery_Power'])

plt.scatter(x=df['Price_Range'],y=df['FC'])

pip install seaborn

sns.jointplot(x='Ram',y='Price_Range',data=df,color='red',kind='kde');

sns.pointplot(y="Int_Memory", x="Price_Range", data=df)

plt.boxplot(df)
plt.show()

"""% of 3g users which supprots the phone"""

labels = ["3G-supported",'Not supported']
values=df['Three_G'].value_counts().values

def standerdize(x):
    return (x - x.mean())/x.std()

fig1, ax1 = plt.subplots()
ax1.pie(values, labels=labels, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()

"""% of Phones which support 4G"""

labels4g = ["4G-supported",'Not supported']
values4g = df['Four_G'].value_counts().values

fig1, ax1 = plt.subplots()
ax1.pie(values4g, labels=labels4g, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()

# battery power vs battery range
sns.boxplot(x="Price_Range", y="Battery_Power", data=df)

# mobile price vs mobile wegiht
sns.jointplot(x='Mobile_W',y='Price_Range',kind='kde',data=df,);

y=df['Price_Range']

y

x=df.drop('Price_Range',axis=1)

x

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

X_train,X_test,Y_train,Y_test=train_test_split(x,y,train_size=0.75,random_state=0)

X_train.shape

X_test.shape

Y_train.shape

Y_test.shape

standerdize_Dataframe = pd.DataFrame()

for c in df.columns[1:]:
    standerdize_Dataframe[c] = standerdize(df[c])

standerdize_Dataframe["Price_Range"] = df["Price_Range"]

standerdize_Dataframe.head()

_ = sns.heatmap(standerdize_Dataframe.corr(),cmap='RdYlGn',fmt = ".2f", annot=True)

from sklearn.decomposition import PCA

pca = PCA(n_components = 5)

pca_model = pca.fit(standerdize_Dataframe[standerdize_Dataframe.columns[:-1]])

X = pca_model.transform(standerdize_Dataframe[standerdize_Dataframe.columns[:-1]])

from sklearn.model_selection import train_test_split

train_x, test_x, train_y, test_y = train_test_split(X,df.Price_Range, test_size=0.2, random_state = 13223, shuffle=True)

model_RMSE = {}

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

lr = LinearRegression()
lr_model = lr.fit(train_x,train_y)
pred = lr_model.predict(test_x)
model_RMSE["Linear Regression"] = mean_squared_error(pred,test_y, squared=False)
print("Root Mean Square Error: {0:.2f}".format(model_RMSE["Linear Regression"]))

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=6, random_state = 34)
rf_model = rf.fit(train_x, train_y)
pred = rf.predict(test_x)
model_RMSE["Random Forest"] = mean_squared_error(pred,test_y, squared=False)
print("Root Mean Square Error: {0:.2f}".format(model_RMSE["Random Forest"]))

from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor(n_neighbors=7)
knn_model = knn.fit(train_x, train_y)
pred = knn_model.predict(test_x)
model_RMSE["K Nearest"] = mean_squared_error(pred,test_y, squared=False)
print("Root Mean Square Error: {0:.2f}".format(model_RMSE["K Nearest"]))

from sklearn.ensemble import GradientBoostingRegressor
gb = GradientBoostingRegressor(learning_rate=0.3,random_state = 124124)
gb_model = gb.fit(train_x, train_y)
pred = gb_model.predict(test_x)
model_RMSE["Gradient Boosting"] = mean_squared_error(pred,test_y, squared=False)
print("Root Mean Square Error: {0:.2f}".format(model_RMSE["Gradient Boosting"]))

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import metrics
from tensorflow.keras.optimizers import RMSprop


l2 =  keras.regularizers.l2(0.001)

model = keras.Sequential([
    keras.layers.Dense(16, activation='relu', input_shape=(5,), kernel_regularizer = l2),
    keras.layers.Dense(8, activation='relu', kernel_regularizer = l2),
    keras.layers.Dense(8, activation='relu', kernel_regularizer = l2),
    keras.layers.Dense(1)
])


opt = RMSprop(learning_rate = 0.01, momentum=0.2)

model.compile(optimizer=opt, loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])


model.fit(train_x, train_y, epochs=500, batch_size=16)


pred = model.predict(test_x)
model_RMSE["Neural Network"] = mean_squared_error(pred,test_y, squared=False)
print("Root Mean Square Error: {0:.2f}".format(model_RMSE["Neural Network"]))

def MeanRegressor(models, weights, X):
    m_p = [0]*len(X)
    inverse_weights = np.ones(len(weights))/weights
    newWeights = inverse_weights / inverse_weights.sum()


    for m,w in zip(models, newWeights):
        m_p += m.predict(X).reshape(len(X))*w

    return m_p/1

pred = MeanRegressor([lr_model,rf_model, knn_model, gb_model, model],list(model_RMSE.values()),test_x)
model_RMSE["Mean Model"] = mean_squared_error(pred,test_y, squared=False)
print("Root Mean Square Error: {0:.2f}".format(model_RMSE["Mean Model"]))

sns.barplot(x = list(model_RMSE.keys()), y = list(model_RMSE.values()))

def pred_pipelin(data):
    for key, value in custom_data.items():
        data[key] = (value - df[key].mean()) / df[key].std()

    decom = pca_model.transform(pd.DataFrame(custom_data, index=[0]))

    return MeanRegressor([lr_model,rf_model, knn_model, gb_model, model],list(model_RMSE.values()),decom)[0]

custom_data = {
 #sales for smart phone
"Price_Range": 2,
"Battery_Power": 150.0,
"Clock_Speed": 2.5,
"FC": 15,
"Int_Memory": 64,
"Mobile_D": 122,
"Mobile_W": 153,
"Cores": 4,
"PC": 10,
"Pixel_H": 895,
"Pixel_W": 1255,
"Screen_W": 15,
"Screen_H": 5,

"Talk_Time": 15,
"Three_G": 1,
"Four_G": 0,
"Touch_Screen": 1,
"Dual_SIM": 1,
"Bluetooth": 1,
"Ram": 1452,
"WiFi": 0,





}

"""MLR"""

X1 = df['Price_Range'] #independent variable
X2 = df['Ram'] #independent variable
Y = df['Battery_Power'] #dependent variable

x1_mean = np.mean(X1)
x2_mean = np.mean(X2)
y_mean = np.mean(Y)
n = X1.count()
Ex1_2 = sum(X1**2) - (sum(X1)**2/n)
Ex2_2 = sum(X2**2) - (sum(X2)**2/n)
Ex1y = sum(X1*Y) - (sum(X1)*sum(Y)/n)
Ex2y = sum(X2*Y) - (sum(X2)*sum(Y)/n)
Ex1x2 = sum(X1*X2) - (sum(X1)*sum(X2)/n)


b1 = ((Ex2_2 * Ex1y) - (Ex1x2 * Ex2y))/((Ex1_2 * Ex2_2) - (Ex1x2**2))
b2 = ((Ex1_2 * Ex2y) - (Ex1x2 * Ex1y))/((Ex1_2 * Ex2_2) - (Ex1x2**2))
b0 = y_mean - (b1*x1_mean) - (b2*x2_mean)

print(b0)
print(b1)
print(b2)

# 5. Find the Ypred
y_pred = b0 + (b1 * X1) + (b2 * X2)
y_pred

# 6. Calculate the SSE (sum of squared error)and RMSE (Root Mean Square Error) value
SSE = sum((Y - y_pred)**2)
print('Sum of squared error:', SSE)

RMSE = np.sqrt(sum((Y - y_pred)**2)/len(X1))
RMSE

# 7. Calculate the coefficient of determination (r2) r-square
SSR = sum((Y - y_pred)**2)
SST = sum((Y - y_mean)**2)
r_square = 1 - (SSR/SST)
r_square

# 9. Predict the output for a given input values
input1 = [float(i) for i in input("Enter the input values 1 to predict output : ").split()]
input2 = [float(i) for i in input("Enter the input values 2 to predict output : ").split()]
print("Input1\tInput2\tOutput")
for i in range(len(input1)):
        output = b0 + (b1 * input1[i]) + (b2 * input2[i])
        print(input1[i],"\t",input2[i],"\t",output)

"""new linear regression

"""

model=LinearRegression()

model.fit(X_train,Y_train)

model.score(X_train,Y_train)

Y_pred=model.predict(X_test)
Y_pred



from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

print(mean_absolute_error(Y_test,Y_pred))
print(mean_absolute_percentage_error(Y_test,Y_pred))
print(mean_squared_error(Y_test,Y_pred))

"""##Logistic Regression
Target variables of the data set are discrete, hence, we are going to apply multiclass logistic regression model.

"""

model_lr = LogisticRegression(multi_class = 'multinomial', solver = 'sag',  max_iter = 10000)

model_lr.fit(X_train,Y_train)

model_lr.intercept_

model_lr.coef_

Y_pred=model_lr.predict(X_test)
Y_pred

from sklearn.metrics import classification_report,accuracy_score,f1_score,confusion_matrix

print(accuracy_score(Y_pred,Y_test))
print(classification_report(Y_pred,Y_test))
print(confusion_matrix(Y_pred,Y_test))

"""KNN neigbohr model is used here"""



model_knn=model_knn.fit(X_train,Y_train)

Y_pred=model_knn.predict(X_test)
Y_pred

from sklearn.metrics import classification_report,accuracy_score,f1_score,confusion_matrix

knn=accuracy_score(Y_pred,Y_test)
print(classification_report(Y_pred,Y_test))
print(confusion_matrix(Y_pred,Y_test))
knn

